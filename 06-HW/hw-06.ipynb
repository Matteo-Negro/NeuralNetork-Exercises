{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteoblack/anaconda3/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/matteoblack/anaconda3/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b\n",
      "  Referenced from: <E03EDA44-89AE-3115-9796-62BA9E0E2EDE> /Users/matteoblack/anaconda3/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <F2FE5CF8-5B5B-3FAD-ADF8-C77D90F49FC9> /Users/matteoblack/anaconda3/lib/python3.11/site-packages/torch/lib/libc10.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/30\n",
      "\n",
      " EPOCH 1/30 \t train loss 0.123 \t val loss 0.065\n",
      "EPOCH 2/30\n",
      "\n",
      " EPOCH 2/30 \t train loss 0.054 \t val loss 0.046\n",
      "EPOCH 3/30\n",
      "\n",
      " EPOCH 3/30 \t train loss 0.041 \t val loss 0.037\n",
      "EPOCH 4/30\n",
      "\n",
      " EPOCH 4/30 \t train loss 0.035 \t val loss 0.034\n",
      "EPOCH 5/30\n",
      "\n",
      " EPOCH 5/30 \t train loss 0.033 \t val loss 0.032\n",
      "EPOCH 6/30\n",
      "\n",
      " EPOCH 6/30 \t train loss 0.031 \t val loss 0.031\n",
      "EPOCH 7/30\n",
      "\n",
      " EPOCH 7/30 \t train loss 0.030 \t val loss 0.030\n",
      "EPOCH 8/30\n",
      "\n",
      " EPOCH 8/30 \t train loss 0.030 \t val loss 0.030\n",
      "EPOCH 9/30\n",
      "\n",
      " EPOCH 9/30 \t train loss 0.029 \t val loss 0.029\n",
      "EPOCH 10/30\n",
      "\n",
      " EPOCH 10/30 \t train loss 0.029 \t val loss 0.029\n",
      "EPOCH 11/30\n",
      "\n",
      " EPOCH 11/30 \t train loss 0.029 \t val loss 0.029\n",
      "EPOCH 12/30\n",
      "\n",
      " EPOCH 12/30 \t train loss 0.028 \t val loss 0.028\n",
      "EPOCH 13/30\n",
      "\n",
      " EPOCH 13/30 \t train loss 0.028 \t val loss 0.028\n",
      "EPOCH 14/30\n",
      "\n",
      " EPOCH 14/30 \t train loss 0.028 \t val loss 0.028\n",
      "EPOCH 15/30\n",
      "\n",
      " EPOCH 15/30 \t train loss 0.028 \t val loss 0.028\n",
      "EPOCH 16/30\n",
      "\n",
      " EPOCH 16/30 \t train loss 0.028 \t val loss 0.028\n",
      "EPOCH 17/30\n",
      "\n",
      " EPOCH 17/30 \t train loss 0.027 \t val loss 0.028\n",
      "EPOCH 18/30\n",
      "\n",
      " EPOCH 18/30 \t train loss 0.027 \t val loss 0.028\n",
      "EPOCH 19/30\n",
      "\n",
      " EPOCH 19/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 20/30\n",
      "\n",
      " EPOCH 20/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 21/30\n",
      "\n",
      " EPOCH 21/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 22/30\n",
      "\n",
      " EPOCH 22/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 23/30\n",
      "\n",
      " EPOCH 23/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 24/30\n",
      "\n",
      " EPOCH 24/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 25/30\n",
      "\n",
      " EPOCH 25/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 26/30\n",
      "\n",
      " EPOCH 26/30 \t train loss 0.027 \t val loss 0.027\n",
      "EPOCH 27/30\n",
      "\n",
      " EPOCH 27/30 \t train loss 0.026 \t val loss 0.027\n",
      "EPOCH 28/30\n",
      "\n",
      " EPOCH 28/30 \t train loss 0.026 \t val loss 0.027\n",
      "EPOCH 29/30\n",
      "\n",
      " EPOCH 29/30 \t train loss 0.026 \t val loss 0.027\n",
      "EPOCH 30/30\n",
      "\n",
      " EPOCH 30/30 \t train loss 0.026 \t val loss 0.027\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/eugeniaring/Medium-Articles/blob/main/Pytorch/denAE.ipynb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np # this module is useful to work with numerical arrays\n",
    "import pandas as pd # this module is useful to work with tabular data\n",
    "import random # this module will be used to select random samples from a collection\n",
    "import os # this module will be used just to create directories in the local filesystem\n",
    "from tqdm import tqdm # this module is useful to plot progress bars\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "data_dir = 'dataset'\n",
    "### With these commands the train and test datasets, respectively, are downloaded\n",
    "### automatically and stored in the local \"data_dir\" directory.\n",
    "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "# fig, axs = plt.subplots(5, 5, figsize=(8,8))\n",
    "# for ax in axs.flatten():\n",
    "#     # random.choice allows to randomly sample from a list-like object (basically anything that can be accessed with an index, like our dataset)\n",
    "#     img, label = random.choice(train_dataset)\n",
    "#     ax.imshow(np.array(img), cmap='gist_gray')\n",
    "#     ax.set_title('Label: %d' % label)\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Set the train transform\n",
    "train_dataset.transform = train_transform\n",
    "# Set the test transform\n",
    "test_dataset.transform = test_transform\n",
    "\n",
    "m = len(train_dataset)\n",
    "\n",
    "#random_split randomly split a dataset into non-overlapping new datasets of given lengths\n",
    "#train (55,000 images), val split (5,000 images)\n",
    "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
    "\n",
    "batch_size=256\n",
    "\n",
    "# The dataloaders handle shuffling, batching, etc...\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        ### Convolutional section\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            # First convolutional layer\n",
    "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "            #nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Second convolutional layer\n",
    "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Third convolutional layer\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "            #nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        ### Flatten layer\n",
    "        #self.flatten = torch.flatten(start_dim=1)\n",
    "\n",
    "        ### Linear section\n",
    "        self.encoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(3 * 3 * 32, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, encoded_space_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply convolutions\n",
    "        x = self.encoder_cnn(x)\n",
    "        # Flatten\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        # # Apply linear layers\n",
    "        x = self.encoder_lin(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoded_space_dim,fc2_input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Linear section\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            # First linear layer\n",
    "            nn.Linear(encoded_space_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            # Second linear layer\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "\n",
    "        ### Convolutional section\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            # First transposed convolution\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            # Second transposed convolution\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            # Third transposed convolution\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply linear layers\n",
    "        x = self.decoder_lin(x)\n",
    "        # Unflatten\n",
    "        x = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))(x)\n",
    "        # Apply transposed convolutions\n",
    "        x = self.decoder_conv(x)\n",
    "        # Apply a sigmoid to force the output to be between 0 and 1 (valid pixel values)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### Initialize the two networks\n",
    "d = 4\n",
    "\n",
    "encoder = Encoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "decoder = Decoder(encoded_space_dim=d,fc2_input_dim=128)\n",
    "\n",
    "### Define the loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "### Define an optimizer (both for the encoder and the decoder!)\n",
    "lr = 0.001 # Learning rate\n",
    "\n",
    "params_to_optimize = [\n",
    "    {'params': encoder.parameters()},\n",
    "    {'params': decoder.parameters()}\n",
    "]\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# print(f'Selected device: {device}')\n",
    "\n",
    "optim = torch.optim.Adam(params_to_optimize, lr=lr)\n",
    "\n",
    "# Move both the encoder and the decoder to the selected device\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "#model.to(device)\n",
    "\n",
    "\n",
    "def add_noise(inputs,noise_factor=0.3):\n",
    "     noise = inputs+torch.randn_like(inputs)*noise_factor\n",
    "     noise = torch.clamp(noise,0.,1.)\n",
    "     return noise\n",
    "\n",
    "\n",
    "### Training function\n",
    "def train_epoch_den(encoder, decoder, device, dataloader, loss_fn, optimizer,noise_factor=0.3):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    train_loss = []\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for image_batch, _ in dataloader: # with \"_\" we just ignore the labels (the second element of the dataloader tuple)\n",
    "        # Move tensor to the proper device\n",
    "        image_noisy = add_noise(image_batch,noise_factor)\n",
    "        image_noisy = image_noisy.to(device)    \n",
    "        # Encode data\n",
    "        encoded_data = encoder(image_noisy)\n",
    "        # Decode data\n",
    "        decoded_data = decoder(encoded_data)\n",
    "        # Evaluate loss\n",
    "        loss = loss_fn(decoded_data, image_batch)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        #print('\\t partial train loss (single batch): %f' % (loss.data))\n",
    "        train_loss.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    return np.mean(train_loss)\n",
    "\n",
    "\n",
    "### Testing function\n",
    "def test_epoch_den(encoder, decoder, device, dataloader, loss_fn,noise_factor=0.3):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        # Define the lists to store the outputs for each batch\n",
    "        conc_out = []\n",
    "        conc_label = []\n",
    "        for image_batch, _ in dataloader:\n",
    "            # Move tensor to the proper device\n",
    "            image_noisy = add_noise(image_batch,noise_factor)\n",
    "            image_noisy = image_noisy.to(device)\n",
    "            # Encode data\n",
    "            encoded_data = encoder(image_noisy)\n",
    "            # Decode data\n",
    "            decoded_data = decoder(encoded_data)\n",
    "            # Append the network output and the original image to the lists\n",
    "            conc_out.append(decoded_data.cpu())\n",
    "            conc_label.append(image_batch.cpu())\n",
    "        # Create a single tensor with all the values in the lists\n",
    "        conc_out = torch.cat(conc_out)\n",
    "        conc_label = torch.cat(conc_label) \n",
    "        # Evaluate global loss\n",
    "        val_loss = loss_fn(conc_out, conc_label)\n",
    "    return val_loss.data\n",
    "\n",
    "\n",
    "def plot_ae_outputs_den(encoder,decoder,n=5,noise_factor=0.3):\n",
    "    plt.figure(figsize=(10,4.5))\n",
    "    for i in range(n):\n",
    "\n",
    "      ax = plt.subplot(3,n,i+1)\n",
    "      img = test_dataset[i][0].unsqueeze(0)\n",
    "      image_noisy = add_noise(img,noise_factor)     \n",
    "      image_noisy = image_noisy.to(device)\n",
    "\n",
    "      encoder.eval()\n",
    "      decoder.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "         rec_img  = decoder(encoder(image_noisy))\n",
    "\n",
    "      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(3, n, i + 1 + n)\n",
    "      plt.imshow(image_noisy.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Corrupted images')\n",
    "\n",
    "      ax = plt.subplot(3, n, i + 1 + n + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.7, \n",
    "                    top=0.9, \n",
    "                    wspace=0.3, \n",
    "                    hspace=0.3)     \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "### Training cycle\n",
    "noise_factor = 0.3\n",
    "num_epochs = 30\n",
    "history_da={'train_loss':[],'val_loss':[]}\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH %d/%d' % (epoch + 1, num_epochs))\n",
    "    ### Training (use the training function)\n",
    "    train_loss=train_epoch_den(\n",
    "        encoder=encoder, \n",
    "        decoder=decoder, \n",
    "        device=device, \n",
    "        dataloader=train_loader, \n",
    "        loss_fn=loss_fn, \n",
    "        optimizer=optim,noise_factor=noise_factor)\n",
    "    \n",
    "    ### Validation  (use the testing function)\n",
    "    val_loss = test_epoch_den(\n",
    "        encoder=encoder, \n",
    "        decoder=decoder, \n",
    "        device=device, \n",
    "        dataloader=valid_loader, \n",
    "        loss_fn=loss_fn,noise_factor=noise_factor)\n",
    "    # Print Validationloss\n",
    "    history_da['train_loss'].append(train_loss)\n",
    "    history_da['val_loss'].append(val_loss)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "    # plot_ae_outputs_den(encoder,decoder,noise_factor=noise_factor)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Put your image generator here\n",
    "random_imgs = list()\n",
    "for i in range(9):\n",
    "    with torch.no_grad():\n",
    "        tensor = torch.rand((1, d))\n",
    "        image  = decoder(tensor).detach().squeeze().numpy()\n",
    "        random_imgs.append(image)\n",
    "\n",
    "img_num = 0\n",
    "matrix = np.zeros((28*3, 28*3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        matrix[i*28 : (i+1)*28, j*28 : (j+1)*28] = random_imgs[img_num]\n",
    "        img_num += 1\n",
    "plt.imshow(matrix, cmap='gist_gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('/Users/matteoblack/Desktop/Proj/proj-NN-2023-2024/06-HW/plot/gen.png', dpi=400, bbox_inches='tight', transparent=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your clustering accuracy calculation here\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X = list()\n",
    "labels = list()\n",
    "for image, label in torch.utils.data.DataLoader(train_data, batch_size=1):\n",
    "    enc = encoder(image).detach().squeeze().numpy()\n",
    "    X.append(enc)\n",
    "    labels.append(label.item())\n",
    "\n",
    "X = np.array(X)\n",
    "labels = np.array(labels)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0, n_init=\"auto\").fit(X)\n",
    "cluster_assignments = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy clustering: 78.47\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = 0\n",
    "for i in range(10):\n",
    "    mask = labels == i\n",
    "    \n",
    "    unique, counts = np.unique(cluster_assignments[mask], return_counts=True)\n",
    "    counts = dict(zip(unique, counts))\n",
    "    tot = 0\n",
    "    tp = 0\n",
    "    for c in counts:\n",
    "        tot += counts[c]\n",
    "        if counts[c] >= tp:\n",
    "            tp = counts[c]\n",
    "    accuracy += tp\n",
    "\n",
    "print(f'Accuracy clustering: {(accuracy/X.shape[0])*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy clustering: 66.74\n",
      "Mapping clustering: [6, 4, 1, 9, 3, 7, 5, 8, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "mapping = np.zeros((10, 10))\n",
    "for i in range(X.shape[0]):\n",
    "    mapping[labels[i].item()][cluster_assignments[i]] += 1\n",
    "\n",
    "matched = list()\n",
    "for i in range(10):\n",
    "    matched.append(np.argmax(mapping[i, :]))\n",
    "\n",
    "i = 0\n",
    "while i < 10:\n",
    "    j = 0\n",
    "    while j < 10:\n",
    "        if matched[i] == matched[j] and i != j:\n",
    "            if mapping[i][int(matched[i])] > mapping[j][int(matched[j])]:\n",
    "                mapping[j][int(matched[j])] = -1\n",
    "                matched[j] = np.argmax(mapping[j, :])\n",
    "                i = -1\n",
    "                break\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "for i, m in enumerate(matched):\n",
    "    accuracy += mapping[i][m]\n",
    "    \n",
    "print(f'Accuracy clustering: {(accuracy/X.shape[0])*100:.2f}%')\n",
    "print(f'Mapping clustering: {matched}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
